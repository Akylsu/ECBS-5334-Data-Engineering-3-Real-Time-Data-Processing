### Spark Structured Streaming

#### Introduction
Data pipeline in its core concept is a chain of different sources and processors in a logical flow that takes source data in its raw form into a process form that can be consumed by the application. There are data sources everywhere: around your enterprise, connected devices, stock prices, connected cars, sensors, user tracking - coming at fast pace. Companies want to get inside and see what is happening with the data that they have in their hands and they want to do it as fast as possible to understand what people are doing with e.g their applications real time. The problem is that it's not possible to start writing the data to a large database or a Hadoop cluster and then applying some transformation logic at the end of the day or week to extract value. Companies want to take the data and extract that value right away as **data value decays with time**.

